# -*- coding: utf-8 -*-
"""Kelompok 3 Teknologi Big Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zsqPjHJtHLmh1_IKjcyGrjCFwHmnypDJ

# Introduction 🙌

Hello🥰
We are **Kelompok 3** whose members are:

*   Edric Boby Tri Raharjo - 164221032
*   Fauziah Hamidah Al-Hanief - 164221055
*   Arkan Syafiq At'taqy - 164221062
*   Nazhifah Firyal Jasmine - 164221083

In this notebook, we're developing predictive models to enhance stores' comprehension of their customers. By doing so, we aim to optimize the effectiveness and efficiency of future promotional initiatives, ultimately leading to a more insightful understanding of customer behavior and preferences🏤🪙

Hope it is useful🙌


---

**About Dataset**

This dataset is collected by stores to help them understand their customers and improve their business strategies. This dataset contains information about customer demographics, shopping behavior, and their interactions with stores. <br>
It can be accessed: [Data](https://www.kaggle.com/competitions/data-analytics-competition-find-it-2024/data)


Further explanation below:

`train_features.csv` - the training set

`test_features.csv` - the test set

Columns

1. **ID**: Nomor identifikasi unik untuk setiap pelanggan.
2. **tahun_kelahiran**: Tahun kelahiran pelanggan.
3. **pendidikan**: Tingkat pendidikan pelanggan (SMP,SMA,Sarjana,Magister,Doktor).
4. **status_pernikahan**: Status pernikahan pelanggan (Sendiri,Rencana Menikah,Menikah,Cerai,Cerai Mati).
5. **pendapatan**: Pendapatan pelanggan dalam rupiah.
6. **jumlah_anak_balita**: Banyaknya anak pelanggan yang masih balita.
7. **jumlah_anak_remaja**: Banyaknya anak pelanggan yang sudah remaja.
8. **terakhir_belanja**: Jumlah hari berlalu setelah terakhir belanja.
9. **belanja_buah**: Biaya yang dikeluarkan untuk belanja buah.
10. **belanja_daging**: Biaya yang dikeluarkan untuk belanja daging.
11. **belanja_ikan**: Biaya yang dikeluarkan untuk belanja ikan.
12. **belanja_kue**: Biaya yang dikeluarkan untuk belanja kue.
13. **pembelian_diskon**: Banyaknya pembelian yang dilakukan saat diskon.
14. **pembelian_web**: Banyaknya pembelian yang dilakukan saat web.
15. **pembelian_toko**: Banyaknya pembelian yang dilakukan saat toko.
16. **keluhan**: 1 - pernah memberikan keluhan, 0 - tidak pernah.
17. **tanggal_menjadi_anggota**: Pertama kali terdaftar sebagai anggota
18. **jumlah_promosi** : (target). Pada promosi ke berapa pelanggan menerima program dari toko, dari total 6 kali promosi yang dilakukan. Nilai 0 berarti tidak menerima sama sekali
19. **usia**: Usia pelanggan berdasarkan tahun kelahiran pelanggan
20. **total_belanja**: Total belanja dari semua kategori produk (buah, daging, ikan, kue) untuk setiap pelanggan.
21. **pengeluaran** : Biaya yang dikeluarkan berdasarkan pendapatan dikurangi total_belanja.
22. **jumlah_anak** : Jumlah anak (balita dan remaja)
23. **kategori_pendapatan** : Kategori berdasarkan pendapatan (Rendah, Menengah Rendah, Menengah, Tinggi, Sangat Tinggi)
24. **pendapatan_per_anak** : Jumlah pendapatan dibagi jumlah anak

# Library
"""

# Library for Data Manipulation.
import pandas as pd
import numpy as np

# Library for Data Visualization.
import seaborn as sns
import matplotlib.pyplot as plt
import altair as alt
import matplotlib.ticker as ticker
sns.set(style="white",font_scale=1.5)
sns.set(rc={"axes.facecolor":"#FFFAF0","figure.facecolor":"#FFFAF0"})
sns.set_context("poster",font_scale = .7)

# Library to overcome Warnings.
import warnings
warnings.filterwarnings('ignore')

# Library to perform Statistical Analysis.
from scipy import stats
from scipy.stats import chi2
from scipy.stats import chi2_contingency

# Library to Display whole Dataset.
pd.set_option("display.max.columns",None)


# pipeline
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer

# Commented out IPython magic to ensure Python compatibility.
# install optuna
# %pip install optuna

# Machine learning algorithms
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier
import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,BatchNormalization,Dropout
import os
from sklearn.base import ClassifierMixin
from sklearn.model_selection import StratifiedKFold, cross_val_predict

#for hypertuning
import optuna
from collections import Counter
from sklearn.model_selection import RandomizedSearchCV,GridSearchCV,RepeatedStratifiedKFold

# for model evaluation
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, classification_report, roc_auc_score, cohen_kappa_score, balanced_accuracy_score, roc_curve

"""# Read Data 📑👀"""

# Read data train
df_train = pd.read_csv('/content/train_features.csv')
df_train.head()

# Read data train labels
df_labels = pd.read_csv('/content/train_labels.csv')
df_labels.head()

# Read data test
df_test = pd.read_csv('/content/test_features.csv')
df_test.head()

# Gabungkan dataframe menggunakan merge
df_train = pd.concat([df_train, df_labels], axis=1)
df_train

"""# Review Dataset 🔎📎📃

## Data Dimension
"""

print("Train dataset shape: ",df_train.shape)
print("Train labels dataset shape: ",df_labels.shape)
print("Test dataset shape: ",df_test.shape)

"""**What we found?💡**

* Train Dataset:

  Terdapat 3817 baris data (entri) dalam dataset tersebut dan
  ada 16 kolom (fitur) yang menyimpan informasi atau atribut tentang setiap entri dalam train dataset.

* Test Dataset:

  Terdapat 3818 baris data (entri) dalam dataset tersebut dan ada 17 kolom (fitur) yang menyimpan informasi atau atribut tentang setiap entri dalam test dataset.

## Information of Dataset 📪
"""

# Information of train dataset
df_train.info()

# Information of train dataset
df_labels.info()

# Information of test dataset
df_test.info()

"""## Column"""

# Identify the data types of columns
column_data_types = df_train.dtypes

# Count the numerical and categorical columns
numerical_count = 0
categorical_count = 0

for column_name, data_type in column_data_types.items():
    if np.issubdtype(data_type, np.number):
        numerical_count += 1
    else:
        categorical_count += 1

# Print the counts
print(f"There are {numerical_count} Numerical Columns in Train dataset")
print(f"There are {categorical_count} Categorical Columns in Train dataset")

"""**Column Grouping📑**

category columns = `['pendidikan','status_pernikahan', 'tanggal_menjadi_anggota']`

binary columns =` ['keluhan']`

numeric columns = `['tahun_kelahiran' ,'pendapatan', 'jumlah_anak_balita', 'jumlah_anak_remaja', 'terakhir_belanja', 'belanja_buah', 'belanja_daging', 'belanja_ikan', 'belanja_kue', 'pembelian_diskon', 'pembelian_web', 'pembelian_toko']`

drop column = `['ID', 'tanggal_menjadi_anggota']`
"""

# Drop columns 'id' and 'tanggal_menjadi_anggotathat is not important for modelling
id = df_test['ID']
cols = ['ID', 'tanggal_menjadi_anggota']
df_test.drop(columns=cols, inplace=True)

"""## Descriptive Statistic 📝"""

# Desc stat of train dataset numeric cols
df_train.describe().T

# Desc stat of test dataset numeric cols
df_labels.describe().T

# Desc stat of test dataset numeric cols
df_test.describe().T

# Desc stat of train dataset cat cols
df_train.describe(include="O").T

# Check unique values of cat cols
cat_cols = df_train.select_dtypes(include="O").columns

for column in cat_cols:
    print('Unique values of ', column, set(df_train[column]))

"""**What we found?💡**

Terdapat noise dan missing value pada kolom `'pendidikan'`dan `'status_pernikahan'`:

* '5'
* nan

## Check Duplicate
"""

print("Duplicates in Train Dataset: ",df_train.duplicated().sum())

"""## Check Missing Value"""

print("Checking Null Values in Train Dataset")
missing_data = df_train.isnull().sum().to_frame().rename(columns={0:"Total No. of Missing Values"})
missing_data["% of Missing Values"] = round((missing_data["Total No. of Missing Values"]/len(df_train))*100,2)
missing_data

"""**What we found?💡**

Terdapat missing value dalam data

# Handling 🪄
"""

cols2 = 'tanggal_menjadi_anggota'
df_train.drop(columns=cols2, inplace=True)
df_train_prep = df_train.copy()

df_test_prep = df_test.copy()

"""**Why we do this**❓

Kami membuat copy dari data train yaitu `df_train_prep` untuk menjaga data train yang asli, data yang kami gunakan untuk proses selanjutnya adalah `df_train_prep`

### Handling cat cols
"""

# handling missing value of country cols
df_train_prep['pendidikan'] = df_train_prep['pendidikan'].fillna('Sarjana')
df_train_prep['status_pernikahan'] = df_train_prep['status_pernikahan'].fillna('Rencana Menikah')

# handling missing value of country cols
df_test_prep['pendidikan'] = df_test_prep['pendidikan'].fillna('Sarjana')
df_test_prep['status_pernikahan'] = df_test_prep['status_pernikahan'].fillna('Rencana Menikah')

"""**Why we do this**❓

Kami mengisi missing value pada **pendidikan** dengan` 'Sarjana'` karena `Sarjana` adalah nilai terbanyak <br>
Kami mengisi missing value pada **status_pernikahan** dengan` 'Rencana Menikah'` karena `Rencana Menikah` adalah nilai terbanyak <br>
Kami mengisi missing value pada **tanggal_menjadi_anggota** dengan` '2013-05-07'` karena `2013-05-07` adalah nilai terbanyak
"""

# handling noise of country cols
df_train_prep['pendidikan'] = df_train_prep['pendidikan'].replace({
    '5': 'Sarjana'
})
df_train_prep['status_pernikahan'] = df_train_prep['status_pernikahan'].replace({
    '5': 'Rencana Menikah'
})

# handling noise of country cols
df_test_prep['pendidikan'] = df_test_prep['pendidikan'].replace({
    '5': 'Sarjana'
})
df_test_prep['status_pernikahan'] = df_test_prep['status_pernikahan'].replace({
    '5': 'Rencana Menikah'
})

"""### Handling binary cols

Kami mengisi missing value binary cols dengan nilai terbanyak pada masing-masing kolom
"""

# handling missing value active_number cols
df_train_prep['keluhan'] = df_train_prep['keluhan'].fillna(0)

# handling missing value active_number cols
df_test_prep['keluhan'] = df_test_prep['keluhan'].fillna(0)

"""## Handling num cols

Kami mengisi missing value numeric cols dengan nilai median
"""

# handling missing value age cols
med_pendapatan = df_train_prep['pendapatan'].median()
df_train_prep['pendapatan'] = df_train_prep['pendapatan'].fillna(med_pendapatan)

# handling missing value age cols
med_pendapatan = df_test_prep['pendapatan'].median()
df_test_prep['pendapatan'] = df_test_prep['pendapatan'].fillna(med_pendapatan)

# handling missing value tenure cols
med_jumlah_anak_balita = df_train_prep['jumlah_anak_balita'].median()
df_train_prep['jumlah_anak_balita'] = df_train_prep['jumlah_anak_balita'].fillna(med_jumlah_anak_balita)

# handling missing value tenure cols
med_jumlah_anak_balita = df_test_prep['jumlah_anak_balita'].median()
df_test_prep['jumlah_anak_balita'] = df_test_prep['jumlah_anak_balita'].fillna(med_jumlah_anak_balita)

# handling missing value balance cols
med_jumlah_anak_remaja = df_train_prep['jumlah_anak_remaja'].median()
df_train_prep['jumlah_anak_remaja'] = df_train_prep['jumlah_anak_remaja'].fillna(med_jumlah_anak_remaja)

# handling missing value balance cols
med_jumlah_anak_remaja = df_test_prep['jumlah_anak_remaja'].median()
df_test_prep['jumlah_anak_remaja'] = df_test_prep['jumlah_anak_remaja'].fillna(med_jumlah_anak_remaja)

# handling missing value products_number cols
med_terakhir_belanja= df_train_prep['terakhir_belanja'].median()
df_train_prep['terakhir_belanja'] = df_train_prep['terakhir_belanja'].fillna(med_terakhir_belanja)

# handling missing value products_number cols
med_terakhir_belanja= df_test_prep['terakhir_belanja'].median()
df_test_prep['terakhir_belanja'] = df_test_prep['terakhir_belanja'].fillna(med_terakhir_belanja)

# handling missing value estimated_salary cols
med_belanja_buah = df_train_prep['belanja_buah'].median()
df_train_prep['belanja_buah'] = df_train_prep['belanja_buah'].fillna(med_belanja_buah)

# handling missing value estimated_salary cols
med_belanja_buah = df_test_prep['belanja_buah'].median()
df_test_prep['belanja_buah'] = df_test_prep['belanja_buah'].fillna(med_belanja_buah)

# handling missing value estimated_salary cols
med_belanja_daging = df_train_prep['belanja_daging'].median()
df_train_prep['belanja_daging'] = df_train_prep['belanja_daging'].fillna(med_belanja_daging)

# handling missing value estimated_salary cols
med_belanja_daging = df_test_prep['belanja_daging'].median()
df_test_prep['belanja_daging'] = df_test_prep['belanja_daging'].fillna(med_belanja_daging)

# handling missing value estimated_salary cols
med_belanja_ikan = df_train_prep['belanja_ikan'].median()
df_train_prep['belanja_ikan'] = df_train_prep['belanja_ikan'].fillna(med_belanja_ikan)

# handling missing value estimated_salary cols
med_belanja_ikan = df_test_prep['belanja_ikan'].median()
df_test_prep['belanja_ikan'] = df_test_prep['belanja_ikan'].fillna(med_belanja_ikan)

# handling missing value estimated_salary cols
med_belanja_kue = df_train_prep['belanja_kue'].median()
df_train_prep['belanja_kue'] = df_train_prep['belanja_kue'].fillna(med_belanja_kue)

# handling missing value estimated_salary cols
med_belanja_kue = df_test_prep['belanja_kue'].median()
df_test_prep['belanja_kue'] = df_test_prep['belanja_kue'].fillna(med_belanja_kue)

# handling missing value estimated_salary cols
med_pembelian_diskon = df_train_prep['pembelian_diskon'].median()
df_train_prep['pembelian_diskon'] = df_train_prep['pembelian_diskon'].fillna(med_pembelian_diskon)

# handling missing value estimated_salary cols
med_pembelian_diskon = df_test_prep['pembelian_diskon'].median()
df_test_prep['pembelian_diskon'] = df_test_prep['pembelian_diskon'].fillna(med_pembelian_diskon)

# handling missing value estimated_salary cols
med_pembelian_web = df_train_prep['pembelian_web'].median()
df_train_prep['pembelian_web'] = df_train_prep['pembelian_web'].fillna(med_pembelian_web)

# handling missing value estimated_salary cols
med_pembelian_web = df_test_prep['pembelian_web'].median()
df_test_prep['pembelian_web'] = df_test_prep['pembelian_web'].fillna(med_pembelian_web)

# handling missing value estimated_salary cols
med_pembelian_toko = df_train_prep['pembelian_toko'].median()
df_train_prep['pembelian_toko'] = df_train_prep['pembelian_toko'].fillna(med_pembelian_toko)

# handling missing value estimated_salary cols
med_pembelian_toko = df_test_prep['pembelian_toko'].median()
df_test_prep['pembelian_toko'] = df_test_prep['pembelian_toko'].fillna(med_pembelian_toko)

"""**Check Missing Value** (again)"""

print("Checking Null Values in Train Prep Dataset")
missing_data = df_train_prep.isnull().sum().to_frame().rename(columns={0:"Total No. of Missing Values"})
missing_data["% of Missing Values"] = round((missing_data["Total No. of Missing Values"]/len(df_train))*100,2)
missing_data

"""✅ Karena missing value dan noise sudah teratasi, maka kita dapat melanjutkan ke tahap selanjutnya

# Exploratory Data Analysis 📊🔎

## Function

**Why we do this**❓

Kami membuat function EDA agar lebih simple dan efisien
"""

# Pie bar plot function
def pie_bar_plot(df, col, attrition_col):
    plt.figure(figsize=(14, 6))

    # Extract value counts for the specified column
    value_counts = df[col].value_counts().sort_index()

    # First subplot: Pie chart
    plt.subplot(1, 2, 1)
    ax1 = value_counts
    plt.title(f"Distribution by {col}", fontweight="black", size=14, pad=15)
    colors = sns.color_palette('Set2', len(ax1))
    plt.pie(ax1.values, labels=ax1.index, autopct="%.1f%%", pctdistance=0.75, startangle=90,
            colors=colors, textprops={"size":14})
    center_circle = plt.Circle((0, 0), 0.4, fc='white')
    fig = plt.gcf()
    fig.gca().add_artist(center_circle)

    # Second subplot: Bar plot
    plt.subplot(1, 2, 2)

    # Convert integer attrition column to 'Yes' and 'No'
    df['attrition_label'] = np.where(df[attrition_col] == 1, 'Yes', 'No')

    value_1 = value_counts
    value_2 = df[df['attrition_label'] == 'Yes'][col].value_counts().sort_index()

    ax2 = np.floor((value_2 / value_1) * 100).values
    sns.barplot(x=value_2.index, y=value_2.values, palette='Set2')
    plt.title(f"Attrition Rate by {col}", fontweight="black", size=14, pad=15)

    for index, value in enumerate(value_2):
        plt.text(index, value, str(value) + " (" + str(int(ax2[index])) + "% )", ha="center", va="bottom", size=10)

    plt.tight_layout()
    plt.show()

# Histogram with hue function
def hist_with_hue(df, col, attrition_col):
    plt.figure(figsize=(13.5, 6))

    # Convert integer attrition column to 'Yes' and 'No'
    df['attrition_label'] = np.where(df[attrition_col] == 1, 'Yes', 'No')

    plt.subplot(1, 2, 1)
    sns.histplot(x=col, hue='attrition_label', data=df, kde=True, palette='Set2')

    # Configure the x-axis to display integer values and center-align the labels
    ax = plt.gca()
    ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))
    plt.xticks(rotation=90, position=(0.5, 0), ha='center')  # Rotate x-axis labels by 90 degrees and center-align

    plt.title(f"Distribution by {col}", fontweight="black", size=14, pad=10)

    plt.subplot(1, 2, 2)
    sns.boxplot(x='attrition_label', y=col, data=df, palette='Set2')
    plt.title(f"Distribution by {col} & {attrition_col}", fontweight="black", size=14, pad=10)

    plt.tight_layout()
    plt.show()

# Bar plot percent function
def count_percent_plot(df, col, attrition_col):

    plt.figure(figsize=(13.5, 8))
    plt.subplot(1, 2, 1)
    value_1 = df[col].value_counts()
    sns.barplot(x=value_1.index, y=value_1.values, order=value_1.index, palette='Set2')
    plt.title(f"Customer by {col}", fontweight="black", size=14, pad=15)
    for index, value in enumerate(value_1.values):
        count_percentage = "{:.1f}%".format((value / len(df)) * 100)
        plt.text(index, value, f"{value} ({count_percentage})", ha="center", va="bottom", size=10)
    plt.xticks(rotation=90)

    # Convert integer attrition column to 'Yes' and 'No'
    df['attrition_label'] = np.where(df[attrition_col] == 1, 'Yes', 'No')

    # Sort the values for the second subplot to match the order of the first subplot
    value_2 = df[df['attrition_label'] == 'Yes'][col].value_counts().reindex(value_1.index)

    plt.subplot(1, 2, 2)
    attrition_rate = (value_2 / value_1 * 100).values
    sns.barplot(x=value_2.index, y=value_2.values, order=value_1.index, palette='Set2')
    plt.title(f"Customer Attrition by {col}", fontweight="black", size=14, pad=15)
    for index, value in enumerate(value_2.values):
        attrition_percentage = "{:.1f}%".format(np.round(attrition_rate[index], 1))
        plt.text(index, value, f"{value} ({attrition_percentage})", ha="center", va="bottom", size=10)
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

"""## Show EDA

### pendidikan
"""

# pendidikan
pie_bar_plot(df_train_prep, 'pendidikan','jumlah_promosi')

"""**What we found?**💡

* Mayoritas pelangganan berpendidikan sarjana
* Attrition (jumlah promosi) terbanyak ada pada sarjana

### status pernikahan
"""

# status pernikahan
pie_bar_plot(df_train_prep, 'status_pernikahan','jumlah_promosi')

"""**What we found?**💡

* pelanggan mayoritas berencana menikah
* Atrition (jumlah promosi) lebih banyak pada pelanggan yang berencana menikah

### tahun kelahiran 🔢
"""

# data ori
hist_with_hue(df_train, 'tahun_kelahiran', 'jumlah_promosi')

"""**What we found?**💡

* Dilihat dari tahun kelahiran pelangganan, lebih banyak promosi yes daripada no
* pelanggan mayoritas kelahiran tahun 1976

### terakhir belanja
"""

# data ori
hist_with_hue(df_train, 'terakhir_belanja', 'jumlah_promosi')

"""**What we found?**💡

* Terakhir belanja memengaruhi jumlah promosi

### belanja buah
"""

# data ori
hist_with_hue(df_train,'belanja_buah','jumlah_promosi')

"""**What we found?**💡

* Belanja buah sedikit mempengaruhi jumlah promosi

### belanja daging
"""

# data prep
hist_with_hue(df_train_prep,'belanja_daging','jumlah_promosi')

"""**What we found?**💡

* Belanja daging sedikit mempengaruhi jumlah promosi

### belanja ikan
"""

hist_with_hue(df_train_prep,'belanja_ikan','jumlah_promosi')

"""**What we found?**💡
* Belanja ikan sedikit mempengaruhi jumlah promosi

### belanja kue
"""

hist_with_hue(df_train_prep,'belanja_kue','jumlah_promosi')

"""**What we found?**💡

* Belanja kue sedikit mempengaruhi jumlah promosi

### jumlah anak balita
"""

count_percent_plot(df_train,'jumlah_anak_balita','jumlah_promosi')

"""**What we found?**💡
* Mayoritas pelangganan tidak memiliki anak balita

### jumlah anak remaja
"""

count_percent_plot(df_train_prep,'jumlah_anak_remaja','jumlah_promosi')

"""**What we found?**💡

* Mayoritas pelanggan tidak memiliki anak remaja

### pembelian diskon
"""

hist_with_hue(df_train,'pembelian_diskon','jumlah_promosi')

"""**What we found?**💡
* pembelian diskon sedikit cenderung no

### pembelian web
"""

hist_with_hue(df_train_prep,'pembelian_web','jumlah_promosi')

"""**What we found?**💡

* Pembelian web banyak cenderung no

### pembelian toko
"""

hist_with_hue(df_train,'pembelian_toko','jumlah_promosi')

"""**What we found?**💡
* pelangganan yang membeli di toko cenderung tidak dapat promosi

### keluhan
"""

count_percent_plot(df_train, 'keluhan', 'jumlah_promosi')

"""**What we found?**💡
* Mayoritas pelanggan tidak memberi keluhan
"""

# data prep
pie_bar_plot(df_train_prep, 'keluhan', 'jumlah_promosi')

"""**What we found?**💡
* Mayoritas pelangganan tidak memberi keluhan
"""

# droping the columns which we have created for analysis purpose
df_train_prep.drop(['attrition_label'],axis = 1, inplace=True)

"""**Why we do this**❓

Attrition label hanya kami gunakan untuk EDA, sehingga lebih baik didrop untuk analisis selanjutnya

# Feature Importance 📌

Kami melakukan uji ANOVA untuk menganalisis apakah kolom/fitur/variabel penting untuk jumlah_promosi

## num cols
"""

num_cols = df_train_prep.select_dtypes(np.number).columns

f_scores = {}
p_values = {}

for column in num_cols:
    f_score, p_value = stats.f_oneway(df_train_prep[column],df_train_prep['jumlah_promosi'])

    f_scores[column] = f_score
    p_values[column] = p_value

plt.figure(figsize=(15,6))
keys = list(f_scores.keys())
values = list(f_scores.values())

sns.barplot(x=keys, y=values)
plt.title("Anova-Test F_scores Comparison", fontweight="black", size=16, pad=15)
plt.xticks(rotation=90)

for index,value in enumerate(values):
    plt.text(index,value,int(value), ha="center", va="bottom", size=14)
plt.show()

anova_data = pd.DataFrame({"Features":keys,"F_Score":values})
anova_data["P_value"] = [format(p, '.20f') for p in list(p_values.values())]
anova_data

"""**What we found?**💡

p-value semua variabel numerik < 0.05 sehingga semua variabel numerik penting atau berpengaruh pada jumlah_promosi

## cat cols
"""

cat_cols = df_train_prep.select_dtypes(include="object").columns.tolist()

chi2_statistic = {}
p_values = {}

# Perform chi-square test for each column
for col in cat_cols:
    contingency_table = pd.crosstab(df_train_prep[col], df_train_prep['jumlah_promosi'])
    chi2, p_value, _, _ = chi2_contingency(contingency_table)
    chi2_statistic[col] = chi2
    p_values[col] = p_value

columns = list(chi2_statistic.keys())
values = list(chi2_statistic.values())

plt.figure(figsize=(16,6))
sns.barplot(x=columns, y=values)
plt.xticks(rotation=90)
plt.title("Chi2 Statistic Value of each Categorical Columns",fontweight="black",size=16,pad=15)
for index,value in enumerate(values):
    plt.text(index,value,round(value,2),ha="center",va="bottom",size=15)

plt.show()

chi_data = pd.DataFrame({"Features":columns,"Chi_2 Statistic":values})
chi_data["P_value"] =  [format(p, '.20f') for p in list(p_values.values())]
chi_data

"""**What we found?**💡

pvalue variabel pendidikan dan status_pernikahan < 0.05 sehingga kedua variabel penting untung jumlah_promosi

# Encoding 0️⃣1️⃣

## Pendidikan
"""

df_train_prep["pendidikan"] = df_train_prep["pendidikan"].replace({"SMP":1 ,"SMA":2, "Sarjana":3,"Magister":4, "Doktor":5})
df_test_prep["pendidikan"] = df_test_prep["pendidikan"].replace({"SMP":1 ,"SMA":2, "Sarjana":3,"Magister":4, "Doktor":5})

"""## Status Pernikahan"""

df_train_prep["status_pernikahan"] = df_train_prep["status_pernikahan"].replace({"Sendiri":1 ,"Rencana Menikah":2, "Menikah":3,"Cerai":4, "Cerai Mati":5})
df_test_prep["status_pernikahan"] = df_test_prep["status_pernikahan"].replace({"Sendiri":1 ,"Rencana Menikah":2, "Menikah":3,"Cerai":4, "Cerai Mati":5})

df_train_prep

"""# Correlation Matrix ⛓"""

plt.figure(figsize=(40,20))
plt.title("Correlation Plot")
sns.heatmap(df_train_prep.corr(),linewidths=5, annot=True, square=True,annot_kws={'size': 10}, cmap=sns.cubehelix_palette(as_cmap=True))

# Calculate the correlation matrix
correlation_matrix = df_train_prep.corr()

# Create a mask to identify the features with a correlation coefficient greater than or equal to 0.75
high_correlation_mask = correlation_matrix >= 0.75

# Identify and list the highly correlated features
highly_correlated_features = []

for feature in high_correlation_mask.columns:
    correlated_with = high_correlation_mask.index[high_correlation_mask[feature]].tolist()
    for correlated_feature in correlated_with:
        if feature != correlated_feature and (correlated_feature, feature) not in highly_correlated_features:
            highly_correlated_features.append((feature, correlated_feature))

# Print the highly correlated features
print("Highly correlated features:")
for feature1, feature2 in highly_correlated_features:
    print(f"{feature1} and {feature2}")

"""**What we found?**💡

tidak ada variabel/fitur dengan korelasi tinggi

# Imbalance ⚖️
"""

#Visualization to show Customer Attrition in Counts.
plt.figure(figsize=(17,6))
plt.subplot(1,2,1)
attrition_counts = df_train_prep["jumlah_promosi"].value_counts()
sns.barplot(x=attrition_counts.index, y=attrition_counts.values, palette='Set2')
plt.title("Customer Attrition Counts", fontweight="black", size=14, pad=15)
for i, v in enumerate(attrition_counts.values):
    plt.text(i, v, v, ha="center", fontsize=14)

#Visualization to show Customer Attrition in Percentage.
plt.subplot(1,2,2)
attrition_rate = df_train_prep["jumlah_promosi"].value_counts(normalize=True) * 100
colors = sns.color_palette('Set2', len(attrition_rate))
plt.pie(attrition_rate, labels=attrition_rate.index, autopct="%.2f%%", textprops={"size":14},
        colors=colors, startangle=90)
center_circle = plt.Circle((0, 0), 0.3, fc='white')
fig = plt.gcf()
fig.gca().add_artist(center_circle)
plt.title("Customer Attrition Rate", fontweight="black", size=14, pad=15)
plt.show()

"""**What we found?**💡

Data tidak balance sehingga harus dilakukan balancing

# Independent vs Dependent
"""

x = df_train_prep.drop(['jumlah_promosi'], axis=1) # Independent variabel
y = df_train_prep[['jumlah_promosi']] # Dependen variabel

x

x.info()

# Balancing Imbalanced Data
import imblearn
from imblearn.over_sampling import SMOTE
smote = SMOTE()
x_smote, y_smote = smote.fit_resample(x, y)
print("Before Smote" , y.value_counts())
print()
print("After Smote" , y_smote.value_counts())

"""# Feature Engineering ⚙️"""

# Fungsi untuk mengkategorikan pendapatan
def categorize_income(income):
    if income <= 83359370.75:
        return 1
    elif 83359370.75 < income <= 117732079.0:
        return 2
    elif 117732079.0 < income <= 148781700.0:
        return 3
    elif 148781700.0 < income <= 1305740000.0:
        return 4
    else:
        return 5

from datetime import datetime
import pandas as pd

def newfeatures(df):
    # Usia Pelanggan
    df['usia'] = datetime.now().year - df['tahun_kelahiran']

    # Total Belanja
    df['total_belanja'] = df[['belanja_buah', 'belanja_daging', 'belanja_ikan', 'belanja_kue']].sum(axis=1)

    # Pengeluaran
    df['pengeluaran'] = df['pendapatan'] - df['total_belanja']

    # Jumlah Anak
    #df['jumlah_anak'] = df[['jumlah_anak_balita', 'jumlah_anak_remaja']].sum(axis=1)

    # Kategori Pendapatan
    df['kategori_pendapatan'] = df['pendapatan'].apply(categorize_income)

    # Drop variabel tahun_kelahiran
    df.drop('tahun_kelahiran', axis=1, inplace=True)
    #df.drop('jumlah_anak_balita', axis=1, inplace=True)
    #df.drop('jumlah_anak_remaja', axis=1, inplace=True)
    #df.drop('keluhan', axis=1, inplace=True)
    return df

"""**Why we do this**❓

kami menambah beberapa variabel untuk memperluas informasi yang didapat
"""

newfeatures(x_smote)
newfeatures(df_test_prep)

"""# Feature Scaling"""

from sklearn.preprocessing import RobustScaler, MinMaxScaler
scaler = RobustScaler()
minmax = MinMaxScaler()

x_smote_scaled = x_smote
df_test_scaled = df_test_prep

x_smote.head()

# Scaled with minmax
columns_to_minmax_scale = ['pendidikan','status_pernikahan','terakhir_belanja','jumlah_anak_remaja','kategori_pendapatan']
x_smote_scaled[columns_to_minmax_scale] = minmax.fit_transform(x_smote_scaled[columns_to_minmax_scale])

# Scaled with Robust
columns_to_RobustScaler = ['pendapatan','jumlah_anak_balita','belanja_buah','belanja_daging','belanja_ikan','belanja_kue','pembelian_diskon','pembelian_web','pembelian_toko','total_belanja','usia','pengeluaran']
x_smote_scaled[columns_to_RobustScaler] = scaler.fit_transform(x_smote_scaled[columns_to_RobustScaler])

"""**Why we do this**❓

Kami melakukan:
* minmax scale pada` ['pendidikan','status_pernikahan','terakhir_belanja','jumlah_anak','kategori_pendapatan']` karena tidak terdapat outlier dan untuk memudahkan analisis
* robust scale pada` ['pendapatan','belanja_buah','belanja_daging','belanja_ikan','belanja_kue','pembelian_diskon','pembelian_web','pembelian_toko','total_belanja','usia','pengeluaran','pendapatan_per_anak']` karena variabel ini terdapat banyak outlier, sehingga tidak dapat digunakan minmax scale yang rentan terhadap outlier
"""

df_test_scaled[columns_to_minmax_scale] = minmax.transform(df_test_scaled[columns_to_minmax_scale])
df_test_scaled[columns_to_RobustScaler] = scaler.transform(df_test_scaled[columns_to_RobustScaler])

"""# Baseline Model Build"""

training_score = []
testing_score = []
precission = []
recall = []
Roc_Auc_score = []
f1_score_ = []

"""# Model Building - Scaled Data"""

from sklearn.model_selection import StratifiedKFold, cross_val_predict

def model_prediction(model, x, y, n_splits=5, random_state=42):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    for train_index, test_index in skf.split(x, y):
        x_train, x_test = x[train_index], x[test_index]
        y_train, y_test = y[train_index], y[test_index]

        model.fit(x_train, y_train)
        x_train_pred = model.predict(x_train)
        x_test_pred = model.predict(x_test)
        y_test_prob = model.predict_proba(x_test)[:, 1]

        a = accuracy_score(y_train, x_train_pred) * 100
        b = accuracy_score(y_test, x_test_pred) * 100
        c = precision_score(y_test, x_test_pred)
        d = recall_score(y_test, x_test_pred)
        e = roc_auc_score(y_test, y_test_prob)
        f = f1_score(y_test, x_test_pred)

        training_score.append(a)
        testing_score.append(b)
        precission.append(c)
        recall.append(d)
        Roc_Auc_score.append(e)
        f1_score_.append(f)

    print("\n------------------------------------------------------------------------")
    print(f"Mean Accuracy_Score of {model} model on Training Data is:", np.mean(training_score))
    print(f"Mean Accuracy_Score of {model} model on Testing Data is:", np.mean(testing_score))
    print(f"Mean Precision Score of {model} model is:", np.mean(precission))
    print(f"Mean Recall Score of {model} model is:", np.mean(recall))
    print(f"Mean ROC_AUC Score of {model} model is:", np.mean(Roc_Auc_score))
    print(f"Mean f1 Score of {model} model is:", np.mean(f1_score_))

    print("\n------------------------------------------------------------------------")
    print(f"Classification Report of {model} model is:")
    y_pred_all = cross_val_predict(model, x, y, cv=skf)
    print(classification_report(y, y_pred_all))

    print("\n------------------------------------------------------------------------")
    print(f"Confusion Matrix of {model} model is:")
    cm = confusion_matrix(y, y_pred_all)
    plt.figure(figsize=(8, 4))
    sns.heatmap(cm, annot=True, fmt="g", cmap="summer")
    plt.show()

y_smote_array = np.array(y_smote)

"""# Model Building - Unscaled Data"""

from sklearn.model_selection import StratifiedKFold, cross_val_predict
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix, classification_report, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

def model_prediction(model, x, y, n_splits=5, random_state=42):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    macro_f_scores = []

    for train_index, test_index in skf.split(x, y):
        x_train, x_test = x.iloc[train_index], x.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        model.fit(x_train, y_train)
        x_test_pred = model.predict(x_test)
        y_test_prob = model.predict_proba(x_test)[:, 1]

        macro_f_scores.append(f1_score(y_test, x_test_pred, average='macro'))

    print("\n------------------------------------------------------------------------")
    print(f"Mean Macro F-Score of {model} model is:", np.mean(macro_f_scores))

"""## Extra Trees"""

model_prediction(ExtraTreesClassifier(), x_smote, y_smote, n_splits=5, random_state=42)

"""## Random Forest"""

model_prediction(RandomForestClassifier(), x_smote, y_smote, n_splits=5, random_state=42)

"""## All Models"""

def model_macro_f_score(model, x_data, y_data, n_splits=5):
    # Use cross_val_predict to get predictions
    predictions = cross_val_predict(model, x_data, y_data, cv=n_splits)
    # Calculate Macro F-Score
    macro_f_score = f1_score(y_data, predictions, average='macro')
    return macro_f_score

# Instantiate different classifiers
et_model = ExtraTreesClassifier(random_state=42)
rf_model = RandomForestClassifier(random_state=42)

# List of models
models = [et_model, rf_model]
model_names = ['Extra Trees', 'Random Forest']

# Iterate over models and calculate Macro F-Score
for model, name in zip(models, model_names):
    macro_f_score = model_macro_f_score(model, x_smote, y_smote)
    print(f"{name} Macro F-Score:", macro_f_score)

"""# Hypertuning Selected Models

## Extra Trees
"""

from sklearn.metrics import f1_score
from sklearn.model_selection import cross_val_predict
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import GridSearchCV

def model_prediction(model, x_data, y_data, metric='accuracy', n_splits=5):
    # Perform cross-validation
    scores = cross_val_score(model, x_data, y_data, cv=n_splits, scoring=metric)
    return scores

# Define the parameter grid
param_grid = {
    'n_estimators': [10, 50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Instantiate the ExtraTreesClassifier
et_model = ExtraTreesClassifier(random_state=42)

# Perform hyperparameter tuning using GridSearchCV
grid_search = GridSearchCV(estimator=et_model, param_grid=param_grid, cv=5)
grid_search.fit(x_smote, y_smote)

# Print the best parameters and the best score
print("Best parameters: ", grid_search.best_params_)
print("Best score: ", grid_search.best_score_)

# Use the model_prediction function with the best ExtraTreesClassifier
best_et_model = grid_search.best_estimator_
et_predictions = cross_val_predict(best_et_model, x_smote, y_smote, cv=5)

# Calculate Macro F-Score
macro_f_score = f1_score(y_smote, et_predictions, average='macro')

# Print the Macro F-Score
print("Macro F-Score:", macro_f_score)

"""# Submission"""

df_test_prep

et_pred = best_et_model.predict(df_test_prep)

submission = pd.DataFrame()
submission['ID'] = id
submission['jumlah_promosi'] = et_pred
submission.head()

# Save the submission file
submission.to_csv('submission_19.csv', index=False)
submission

"""🎊🎉**WELL DONE**🎊🎉


---










"""